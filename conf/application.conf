# https://www.playframework.com/documentation/latest/Configuration
{
  persistence {
    user  = "myUser"
    pass  = ""
    host  = "127.0.0.1"
    port  = 0
    ssl   = false
    name  = "dbName"
  }
  # Alternative DB manager we are migrating to, will auto handle thread / connection pools.
  slick.dbs.default.profile="slick.jdbc.PostgresProfile$"
  slick.dbs.default.numThreads = 15
  slick.dbs.default.db.dataSourceClass = "slick.jdbc.DatabaseUrlDataSource"
  slick.dbs.default.db.properties.driver = "org.postgresql.Driver"
  slick.dbs.default.db.properties.url="jdbc:postgresql://localhost:0000/postgres?currentSchema=public"
  # ?currentSchema=postgres&user=postgres&password=postgres"
  slick.dbs.default.db.properties.user = "usr"
  slick.dbs.default.db.properties.password = "pass"

  node {
    url = "127.0.0.1"
    key = ""
    storagePath = ""
    pass        = ""
    networkType = "MAINNET"
    explorerURL = "default"
  }

  params {
    # Number of blocks to evaluate for groups. TODO: Rename this
    pendingBlockNum  = 3
    # Number of blocks to validate (transfer from MC to SubPooling db, then check if block exists on-chain)
    numToValidate    = 10
    # Number of confirmations for each block
    confirmationNum  = 20
    # Base path to load .ergoscript files from
    scriptBasePath   = ""
    # PPLNS Window
    pplnsWindow      = 0.5
    # Number to multiply each share score by. Increasing this number means a finer granularity / more decimal points in payouts, at the cost of
    # potential integer / long overflows in the smart contracts. Use caution when increasing.
    scoreAdjustCoeff = 10000000

    # MUST CHANGE TO REAL VALUE IN PRODUCTION
    defaultMinerPK   = "02055e8146e7356b3abce254803ed4f0b7d10235cc3161186a7a4fab58c01ef706" # Used in case node has errors with certain api requests
    # CHANGE TO FALSE IN PRODUCTION
    autoConfirmGroups = false
    # Make placements for the same pool parallel to further increase speed of payouts. - Experimental, turn off in prod
    parallelPoolPlacements = false

    # How long after getting a failed(or unfound) transaction should the transaction be executed again.
    # Be careful with low times(especially on placements), race conditions are possible between transactions that are still confirming
    restartPlacements = 24 hours
    restartDists      = 60 minutes

    # Amount of ERG to pre-collect for distributions. Not what is actually spent, so try to overestimate.
    # In general, pendingBlockNum * blockReward + pendingBlockNum * 2 ERG is a good place to start
    # Value is given in ERG and converted to nanoErgs at compile time.
    amountToPreCollect = 100000

    # Token for BlokBot
    blockBotToken = "none"
    # To find this, go into web telegram z, and get the id from the url. Make sure to add -100 in front of it
    blockBotChat = "none"
    enableBlockBot = true

    # Default Pool for miners to connect to
    defaultPool = "default"

    # These windows are in numbers of weeks. So a value of 2 will mean that share or minerstats entries that are older
    # than 2 weeks will be moved to archives and deleted
    keepSharesWindow = 2
    keepMinerStatsWindow = 2

    # Fee parameters
    feeAddress = ""
    # 1.0 = 1% fees, .01 = .01% fees
    feePercent = 1.0

    # Regenerate from blockchain. Turn this on carefully, only needed when stuck on processed block.
    regenFromChain = false
    regenPlaceTx = ""
    regenPlaceBlock = 0
    regenStatePool = ""
    regenType = ""
    singularGroups = true
    # Number to start groups at. Only relevant numbers are 0 or 1. 0 = placements first. 1 = dist first.
    groupStart = 0

    sendTxs = true

    plasmaStorage = "/opt/subpooling/plasma"
  }
  play.http.secret.key="changeme"
  play.filters.hosts {
    allowed = ["localhost:9000"]
  }

  swagger.api.basepath = "/api"

  # ======Actor + Thread Handling=======

  # These are actor settings, allowing you to dictate the number of actors handling requests
  # Increasing the numbers enables more parallelization at the cost of more thread usage
  # within each actor's assigned thread pools

  subpool-actors {
    quick-query-readers {
      num  = 5
    }
    explorer-handlers {
      num  = 5
    }
    blocking-db-writers {
      num  = 5
    }
    group-req-handlers {
      num  = 5
    }
  }

  # These tasks represent different actions the application may perform and at what intervals they happen
  subpool-tasks {
    # Changes blocks from pending to confirmed status
    init-pool-task {
      enabled  = false
      startup  = 30 seconds
      interval = 8 minutes
    }

    # Changes blocks from pending to confirmed status
    block-status-check {
      enabled  = true
      startup  = 10 seconds
      interval = 2 minutes
    }
    # Changes pools from confirmed to initiated and success status, while changing blocks from
    # confirmed to processing status. Equivalent to payout manager with added actions
    group-execution {
      enabled  = true
      startup  = 30 seconds
      interval = 12 minutes
    }
    # Cross checks database status of blocks and pools pending some form of confirmation to the next state.
    # For blocks, db cross check will change the following statuses (processing -> processed), (initiated -> paid)

    db-cross-check {
      enabled  = true
      startup  = 30 seconds
      interval = 2 minutes
    }
    # Queries blocks table every interval and immediately inserts the block into subpool_blocks
    # BE CAREFUL IF MAKING THIS LONGER
    pool-block-listener {
      enabled  = true
      startup  = 30 seconds
      interval = 2 minute
    }
  }


  subpool-contexts {

    group-dispatcher {
      type = Dispatcher
      executor = "thread-pool-executor"
      thread-pool-executor {
        fixed-pool-size = 32
      }
      throughput = 1
}

    task-dispatcher {
      type = Dispatcher
      executor = "thread-pool-executor"
      thread-pool-executor {
        fixed-pool-size = 32
      }
      throughput = 1
    }
    # Dynamic dispatcher for quick queries that grows and shrinks with load
    quick-query-dispatcher {
        type = Dispatcher
        # What kind of ExecutionService to use
        executor = "fork-join-executor"
        # Configuration for the fork join pool
        fork-join-executor {
          # Min number of threads to cap factor-based parallelism number to
          parallelism-min = 10
          # Parallelism (threads) ... ceil(available processors * factor)
          parallelism-factor = 2.0
          # Max number of threads to cap factor-based parallelism number to
          parallelism-max = 20
        }
        # Throughput defines the maximum number of messages to be
        # processed per actor before the thread jumps to the next actor.
        # Set to 1 for as fair as possible (1 message per actor)
        throughput = 1
    }
    # Dispatcher used for blocking calls, like third party API calls (explorer), database writes, and extremely large
    # database queries (such as share collection)
    blocking-io-dispatcher {
        type = Dispatcher
        executor = "thread-pool-executor"
        thread-pool-executor {
          fixed-pool-size = 32
        }
        throughput = 1
    }
  }


}